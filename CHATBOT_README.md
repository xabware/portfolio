# Chatbot con LLM Local

Este proyecto ahora incluye un chatbot con un modelo de lenguaje (LLM) que se ejecuta completamente en el navegador, sin necesidad de un backend.

## ğŸš€ CaracterÃ­sticas

- **EjecuciÃ³n local**: El modelo se ejecuta completamente en tu navegador usando WebGPU
- **Sin backend**: No requiere llamadas a API ni servidores externos
- **Privacidad**: Todas las conversaciones permanecen en tu dispositivo
- **Modelo ligero**: Usa Phi-3.5-mini, optimizado para navegadores

## ğŸ“‹ Requisitos

Para que el chatbot funcione correctamente, necesitas:

1. **Navegador compatible con WebGPU**:
   - Chrome 113+ o Edge 113+
   - Habilitar WebGPU en `chrome://flags` si es necesario

2. **ConexiÃ³n a internet** (solo la primera vez):
   - El modelo se descarga una vez (~2GB)
   - DespuÃ©s se almacena en cachÃ© y funciona offline

## ğŸ¯ Uso

1. Navega a la secciÃ³n "Chat" en el portfolio
2. Espera a que el modelo se cargue (puede tardar 2-5 minutos la primera vez)
3. Una vez cargado, puedes hacer preguntas al asistente virtual
4. El modelo responde en tiempo real usando IA local

## âš™ï¸ ConfiguraciÃ³n TÃ©cnica

### TecnologÃ­as Utilizadas

- **@mlc-ai/web-llm**: Framework para ejecutar LLMs en el navegador
- **Phi-3.5-mini-instruct**: Modelo de lenguaje pequeÃ±o y eficiente
- **WebGPU**: Para aceleraciÃ³n de hardware
- **React + TypeScript**: Frontend

### Archivos Clave

- `src/hooks/useWebLLM.ts`: Hook personalizado para gestionar el modelo
- `src/components/Chatbot.tsx`: Componente principal del chat
- `vite.config.ts`: ConfiguraciÃ³n para soportar WebGPU y Workers

## ğŸ”§ Desarrollo

Para ejecutar el proyecto en desarrollo:

\`\`\`bash
npm install
npm run dev
\`\`\`

El servidor incluye los headers necesarios para SharedArrayBuffer y WebGPU.

## ğŸ“ Notas Importantes

1. **Primera carga**: La primera vez que uses el chat, el modelo se descargarÃ¡ (~2GB). Esto puede tardar varios minutos dependiendo de tu conexiÃ³n.

2. **Memoria**: El modelo requiere aproximadamente 4GB de RAM disponible.

3. **Rendimiento**: El rendimiento varÃ­a segÃºn tu hardware. Los dispositivos con GPU dedicada tendrÃ¡n mejor rendimiento.

4. **Compatibilidad**: Si tu navegador no soporta WebGPU, verÃ¡s un mensaje de error con instrucciones.

## ğŸ¨ PersonalizaciÃ³n

Puedes personalizar el modelo editando `src/hooks/useWebLLM.ts`:

- **Modelo**: Cambia `'Phi-3.5-mini-instruct-q4f16_1-MLC'` por otro modelo compatible
- **Temperatura**: Ajusta `temperature` (0.0 a 1.0) para respuestas mÃ¡s o menos creativas
- **Tokens**: Modifica `max_tokens` para respuestas mÃ¡s largas o cortas
- **System Prompt**: Edita el prompt del sistema para cambiar el comportamiento del asistente

## ğŸ†š ComparaciÃ³n: Local vs Backend

### LLM Local (Actual)

âœ… Sin latencia de red
âœ… Total privacidad
âœ… Funciona offline despuÃ©s de la primera carga
âœ… Sin costos de servidor
âŒ Requiere descarga inicial grande
âŒ Limitado por hardware del cliente

### Backend RAG (Anterior)

âœ… Modelos mÃ¡s potentes
âœ… Sin requisitos de hardware del cliente
âœ… Actualizaciones instantÃ¡neas
âŒ Requiere servidor y costos
âŒ Latencia de red
âŒ Datos enviados a servidores externos

## ğŸ› SoluciÃ³n de Problemas

### El modelo no carga

- Verifica que tu navegador soporte WebGPU
- Limpia el cachÃ© del navegador
- AsegÃºrate de tener suficiente espacio en disco

### Respuestas lentas

- El hardware de tu dispositivo puede ser limitado
- Cierra otras pestaÃ±as para liberar memoria
- Considera usar un modelo mÃ¡s pequeÃ±o

### Error de WebGPU

- Actualiza tu navegador a la Ãºltima versiÃ³n
- Habilita WebGPU en las flags del navegador
- Verifica que tu GPU sea compatible

## ğŸ“š Recursos

- [WebLLM Documentation](https://mlc.ai/web-llm/)
- [WebGPU Support](https://caniuse.com/webgpu)
- [Phi-3 Model Info](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
